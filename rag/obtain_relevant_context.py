#!/usr/bin/env python3
"""
è·å–ä¸é—®é¢˜ç›¸å…³çš„ä¸Šä¸‹æ–‡å—è„šæœ¬
åŸºäºå·²æ„å»ºçš„å‘é‡æ•°æ®åº“ï¼Œä¸ºissues.jsonä¸­çš„æ¯ä¸ªé—®é¢˜è·å–ç›¸å…³ä¸Šä¸‹æ–‡
"""

import json
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import logging
from collections import defaultdict

# ä»åŒç›®å½•å¯¼å…¥RAGç³»ç»Ÿ
from rag_system import LangChainRAGSystem


class RelevantContextRetriever:
    """ç›¸å…³ä¸Šä¸‹æ–‡æ£€ç´¢å™¨"""

    def __init__(
        self,
        vector_db_dir: str = "./vector_dbs",
        issues_file: str = "../evalulate/datasets/issues.json",
        output_dir: str = "./relevant_context",
    ):
        """
        åˆå§‹åŒ–æ£€ç´¢å™¨

        Args:
            vector_db_dir: å‘é‡æ•°æ®åº“ç›®å½•
            issues_file: issues.jsonæ–‡ä»¶è·¯å¾„
            output_dir: ç›¸å…³ä¸Šä¸‹æ–‡è¾“å‡ºç›®å½•
        """
        self.vector_db_dir = Path(vector_db_dir)
        self.issues_file = Path(issues_file)
        self.output_dir = Path(output_dir)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(exist_ok=True)

        # è®¾ç½®æ—¥å¿—
        self._setup_logging()

        # åŠ è½½é—®é¢˜æ•°æ®
        self.issues = self._load_issues()

        # æŒ‰ä»“åº“åˆ†ç»„é—®é¢˜
        self.issues_by_repo = self._group_issues_by_repo()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_issues": len(self.issues),
            "total_repos": len(self.issues_by_repo),
            "processed_issues": 0,
            "failed_issues": 0,
            "processing_errors": [],
        }

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler("obtain_relevant_context.log"),
                logging.StreamHandler(),
            ],
        )
        self.logger = logging.getLogger(__name__)

    def _load_issues(self) -> List[Dict]:
        """åŠ è½½issues.jsonæ–‡ä»¶"""
        if not self.issues_file.exists():
            self.logger.error(f"Issuesæ–‡ä»¶ä¸å­˜åœ¨: {self.issues_file}")
            return []

        try:
            with open(self.issues_file, "r", encoding="utf-8") as f:
                issues = json.load(f)

            self.logger.info(f"æˆåŠŸåŠ è½½ {len(issues)} ä¸ªé—®é¢˜")
            return issues
        except Exception as e:
            self.logger.error(f"åŠ è½½issuesæ–‡ä»¶å¤±è´¥: {e}")
            return []

    def _group_issues_by_repo(self) -> Dict[str, List[Dict]]:
        """æŒ‰ä»“åº“åˆ†ç»„é—®é¢˜"""
        grouped = defaultdict(list)

        for issue in self.issues:
            repo = issue.get("repo")
            if repo:
                # ä»repoå…¨åæå–ä»“åº“å (e.g., "square/okhttp" -> "okhttp")
                repo_name = repo.split("/")[-1] if "/" in repo else repo
                grouped[repo_name].append(issue)

        self.logger.info(f"é—®é¢˜åˆ†å¸ƒåœ¨ {len(grouped)} ä¸ªä»“åº“ä¸­:")
        for repo_name, issues in grouped.items():
            self.logger.info(f"  - {repo_name}: {len(issues)} ä¸ªé—®é¢˜")

        return dict(grouped)

    def _construct_query(self, issue: Dict) -> str:
        """æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²"""
        title = issue.get("issue_title", "")
        body = issue.get("issue_body", "")

        # æ‹¼æ¥æ ‡é¢˜å’Œæ­£æ–‡
        query = ""
        if title:
            query += title
        if body:
            if query:
                query += "\n\n"
            query += body

        return query.strip()

    def _load_rag_system(
        self, repo_name: str, version: str
    ) -> Optional[LangChainRAGSystem]:
        """åŠ è½½æŒ‡å®šä»“åº“å’Œç‰ˆæœ¬çš„RAGç³»ç»Ÿ"""
        try:
            collection_name = f"{repo_name}_{version}"
            persist_dir = (
                self.vector_db_dir / f"repository-{repo_name}" / f"chroma_db_{version}"
            )

            if not persist_dir.exists():
                self.logger.warning(f"å‘é‡æ•°æ®åº“ä¸å­˜åœ¨: {persist_dir}")
                return None

            rag_system = LangChainRAGSystem(collection_name)
            rag_system.persist_directory = str(persist_dir)

            # éªŒè¯æ˜¯å¦å¯ä»¥åŠ è½½
            test_results = rag_system.search("test", k=1)
            if test_results is None:
                self.logger.warning(f"æ— æ³•åŠ è½½å‘é‡æ•°æ®åº“: {persist_dir}")
                return None

            return rag_system

        except Exception as e:
            self.logger.error(f"åŠ è½½RAGç³»ç»Ÿå¤±è´¥ {repo_name}/{version}: {e}")
            return None

    def _search_relevant_context(self, query: str, repo_name: str, k: int = 5) -> Dict:
        """æœç´¢ç›¸å…³ä¸Šä¸‹æ–‡"""
        results = {}

        # ä¸‰ç§ç‰ˆæœ¬çš„RAGç³»ç»Ÿ
        versions = ["basic", "enhanced", "compressed"]

        for version in versions:
            rag_system = self._load_rag_system(repo_name, version)

            if rag_system:
                try:
                    search_results = rag_system.search(query, k=k)
                    results[version] = search_results
                    self.logger.debug(
                        f"ä»“åº“ {repo_name} {version}ç‰ˆæœ¬: æ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ"
                    )
                except Exception as e:
                    self.logger.error(f"æœç´¢å¤±è´¥ {repo_name}/{version}: {e}")
                    results[version] = []
            else:
                results[version] = []

        return results

    def process_single_issue(self, issue: Dict, repo_name: str) -> bool:
        """å¤„ç†å•ä¸ªé—®é¢˜"""
        issue_id = issue.get("id", "unknown")
        issue_title = issue.get("issue_title", "")

        self.logger.info(f"å¤„ç†é—®é¢˜: {issue_id} - {issue_title[:50]}...")

        try:
            # æ„å»ºæŸ¥è¯¢
            query = self._construct_query(issue)

            if not query:
                self.logger.warning(f"é—®é¢˜ {issue_id}: æŸ¥è¯¢å†…å®¹ä¸ºç©º")
                return False

            # æœç´¢ç›¸å…³ä¸Šä¸‹æ–‡
            search_results = self._search_relevant_context(query, repo_name, k=20)

            # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆåœ¨ç‹¬ç«‹çš„relevant_contextç›®å½•ä¸‹ï¼‰
            output_dir = self.output_dir / issue_id
            output_dir.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜ç»“æœ
            output_file = output_dir / "rag_comparison_results.json"
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(search_results, f, ensure_ascii=False, indent=2)

            self.logger.info(f"âœ… é—®é¢˜ {issue_id} å¤„ç†å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_file}")
            self.stats["processed_issues"] += 1
            return True

        except Exception as e:
            self.logger.error(f"âŒ é—®é¢˜ {issue_id} å¤„ç†å¤±è´¥: {e}")
            self.stats["failed_issues"] += 1
            self.stats["processing_errors"].append(
                {
                    "issue_id": issue_id,
                    "repo_name": repo_name,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat(),
                }
            )
            return False

    def process_repo_issues(self, repo_name: str) -> Dict:
        """å¤„ç†å•ä¸ªä»“åº“çš„æ‰€æœ‰é—®é¢˜"""
        issues = self.issues_by_repo.get(repo_name, [])

        if not issues:
            self.logger.warning(f"ä»“åº“ {repo_name}: æ²¡æœ‰æ‰¾åˆ°é—®é¢˜")
            return {"processed": 0, "failed": 0}

        # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å­˜åœ¨
        repo_db_dir = self.vector_db_dir / f"repository-{repo_name}"
        if not repo_db_dir.exists():
            self.logger.warning(f"ä»“åº“ {repo_name}: å‘é‡æ•°æ®åº“ç›®å½•ä¸å­˜åœ¨")
            return {"processed": 0, "failed": len(issues)}

        self.logger.info(f"å¼€å§‹å¤„ç†ä»“åº“ {repo_name} çš„ {len(issues)} ä¸ªé—®é¢˜")

        processed = 0
        failed = 0

        for issue in issues:
            if self.process_single_issue(issue, repo_name):
                processed += 1
            else:
                failed += 1

        self.logger.info(f"ä»“åº“ {repo_name} å¤„ç†å®Œæˆ: æˆåŠŸ {processed}, å¤±è´¥ {failed}")
        return {"processed": processed, "failed": failed}

    def process_all_issues(self) -> Dict:
        """å¤„ç†æ‰€æœ‰é—®é¢˜"""
        self.logger.info("å¼€å§‹å¤„ç†æ‰€æœ‰é—®é¢˜...")

        if not self.vector_db_dir.exists():
            self.logger.error(f"å‘é‡æ•°æ®åº“ç›®å½•ä¸å­˜åœ¨: {self.vector_db_dir}")
            return self.stats

        repo_results = {}

        for repo_name in self.issues_by_repo.keys():
            self.logger.info(f"å¤„ç†ä»“åº“: {repo_name}")
            repo_results[repo_name] = self.process_repo_issues(repo_name)

        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.logger.info("=" * 60)
        self.logger.info("å¤„ç†å®Œæˆ! ç»Ÿè®¡æŠ¥å‘Š:")
        self.logger.info(f"æ€»é—®é¢˜æ•°: {self.stats['total_issues']}")
        self.logger.info(f"æ€»ä»“åº“æ•°: {self.stats['total_repos']}")
        self.logger.info(f"æˆåŠŸå¤„ç†: {self.stats['processed_issues']}")
        self.logger.info(f"å¤±è´¥é—®é¢˜: {self.stats['failed_issues']}")

        if self.stats["processing_errors"]:
            self.logger.info("\nå¤±è´¥çš„é—®é¢˜:")
            for error in self.stats["processing_errors"]:
                self.logger.info(
                    f"  - {error['issue_id']} ({error['repo_name']}): {error['error']}"
                )

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = self.output_dir / "context_retrieval_statistics.json"
        final_stats = {
            **self.stats,
            "repo_results": repo_results,
            "completion_time": datetime.now().isoformat(),
        }

        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(final_stats, f, ensure_ascii=False, indent=2)

        self.logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
        return final_stats


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ” ç›¸å…³ä¸Šä¸‹æ–‡æ£€ç´¢å·¥å…·")
    print("=" * 60)

    # æ£€æŸ¥ä¾èµ–
    try:
        import importlib.util

        spec = importlib.util.find_spec("rag_system")
        if spec is None:
            raise ImportError("rag_system module not found")
        print("âœ… ä¾èµ–æ£€æŸ¥é€šè¿‡")
    except ImportError as e:
        print(f"âŒ ä¾èµ–æ£€æŸ¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿rag_system.pyåœ¨åŒä¸€ç›®å½•ä¸‹ï¼Œå¹¶å·²æ„å»ºå‘é‡æ•°æ®åº“")
        return

    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = RelevantContextRetriever()

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not retriever.issues:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„é—®é¢˜æ•°æ®")
        return

    if not retriever.issues_by_repo:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä»“åº“é—®é¢˜")
        return

    print(
        f"ğŸ“Š å¾…å¤„ç†: {retriever.stats['total_issues']} ä¸ªé—®é¢˜ï¼Œåˆ†å¸ƒåœ¨ {retriever.stats['total_repos']} ä¸ªä»“åº“ä¸­"
    )

    # æ‰§è¡Œå¤„ç†
    stats = retriever.process_all_issues()

    print("\n" + "=" * 60)
    print("ğŸ‰ å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š æˆåŠŸ: {stats['processed_issues']}, å¤±è´¥: {stats['failed_issues']}")

    print("\nğŸ“ è¾“å‡ºç›®å½•ç»“æ„:")
    output_dir = Path("./relevant_context")
    if output_dir.exists():
        for repo_dir in sorted(output_dir.iterdir()):
            if repo_dir.is_dir():
                print(f"  ğŸ“‚ {repo_dir.name}/")
                issue_dirs = [d for d in repo_dir.iterdir() if d.is_dir()]
                for issue_dir in sorted(issue_dirs)[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    print(f"    ğŸ“ {issue_dir.name}/")
                    print("      ğŸ“„ rag_comparison_results.json")
                if len(issue_dirs) > 3:
                    print(f"    ... å’Œå…¶ä»– {len(issue_dirs) - 3} ä¸ªé—®é¢˜ç›®å½•")

    print("\nğŸ“ æ—¥å¿—æ–‡ä»¶: obtain_relevant_context.log")
    print("ğŸ“ˆ ç»Ÿè®¡æ–‡ä»¶: ./relevant_context/context_retrieval_statistics.json")


if __name__ == "__main__":
    main()
