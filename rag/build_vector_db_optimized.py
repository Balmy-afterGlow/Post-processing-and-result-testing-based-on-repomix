#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆæ‰¹é‡å‘é‡æ•°æ®åº“æ„å»ºè„šæœ¬
é’ˆå¯¹å¤§å‹ä»“åº“è¿›è¡Œäº†æ€§èƒ½ä¼˜åŒ–ï¼šæ–‡æ¡£è¿‡æ»¤ã€æ‰¹å¤„ç†ã€æ™ºèƒ½é‡‡æ ·
"""

import os
import json
import shutil
from pathlib import Path
from typing import List, Dict
from datetime import datetime
import logging

# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œé¿å…ç½‘ç»œè¯·æ±‚
os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_DATASETS_OFFLINE"] = "1"

# ä»åŒç›®å½•å¯¼å…¥RAGç³»ç»Ÿ
from rag_system import MarkdownParser, DocumentProcessor, LangChainRAGSystem


class OptimizedVectorDatabaseBuilder:
    """ä¼˜åŒ–ç‰ˆå‘é‡æ•°æ®åº“æ‰¹é‡æ„å»ºå™¨"""

    def __init__(
        self,
        repomix_dir: str = "../repomix_md",
        output_dir: str = "./vector_dbs_optimized",
        max_documents_per_version: int = 2000,  # æ¯ä¸ªç‰ˆæœ¬æœ€å¤§æ–‡æ¡£æ•°
        min_content_length: int = 100,  # æœ€å°å†…å®¹é•¿åº¦
        batch_size: int = 100,  # æ‰¹å¤„ç†å¤§å°
        skip_large_files: bool = True,  # è·³è¿‡å¤§æ–‡ä»¶
        max_file_size_mb: float = 5.0,  # æœ€å¤§æ–‡ä»¶å¤§å°(MB)
    ):
        """
        åˆå§‹åŒ–ä¼˜åŒ–æ„å»ºå™¨

        Args:
            repomix_dir: repomixè¾“å‡ºç›®å½•
            output_dir: å‘é‡æ•°æ®åº“è¾“å‡ºç›®å½•
            max_documents_per_version: æ¯ä¸ªç‰ˆæœ¬æœ€å¤§æ–‡æ¡£æ•°é‡
            min_content_length: æœ€å°å†…å®¹é•¿åº¦è¿‡æ»¤
            batch_size: æ‰¹å¤„ç†å¤§å°
            skip_large_files: æ˜¯å¦è·³è¿‡å¤§æ–‡ä»¶
            max_file_size_mb: æœ€å¤§å¤„ç†æ–‡ä»¶å¤§å°(MB)
        """
        self.repomix_dir = Path(repomix_dir)
        self.output_dir = Path(output_dir)
        self.max_documents_per_version = max_documents_per_version
        self.min_content_length = min_content_length
        self.batch_size = batch_size
        self.skip_large_files = skip_large_files
        self.max_file_size_mb = max_file_size_mb

        self.parser = MarkdownParser()
        self.processor = DocumentProcessor()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(exist_ok=True)

        # è®¾ç½®æ—¥å¿—
        self._setup_logging()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_repos": 0,
            "processed_repos": 0,
            "skipped_repos": 0,
            "failed_repos": 0,
            "total_documents": 0,
            "filtered_documents": 0,
            "processing_errors": [],
        }

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler("optimized_vector_db_build.log"),
                logging.StreamHandler(),
            ],
        )
        self.logger = logging.getLogger(__name__)

    def filter_and_prioritize_documents(
        self, documents: List, version_name: str
    ) -> List:
        """
        è¿‡æ»¤å’Œä¼˜å…ˆæ’åºæ–‡æ¡£

        Args:
            documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            version_name: ç‰ˆæœ¬åç§°

        Returns:
            è¿‡æ»¤å’Œæ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        # 1. æŒ‰å†…å®¹é•¿åº¦è¿‡æ»¤
        filtered_docs = [
            doc
            for doc in documents
            if len(doc.page_content.strip()) >= self.min_content_length
        ]

        original_count = len(documents)
        after_filter_count = len(filtered_docs)

        self.logger.info(
            f"{version_name}: é•¿åº¦è¿‡æ»¤ {original_count} -> {after_filter_count} ä¸ªæ–‡æ¡£"
        )

        # 2. æŒ‰é‡è¦æ€§æ’åº
        def get_document_priority(doc):
            priority = 0
            content = doc.page_content.lower()
            metadata = doc.metadata

            # é‡è¦æ–‡ä»¶ç±»å‹ä¼˜å…ˆ
            important_extensions = [
                ".py",
                ".js",
                ".ts",
                ".java",
                ".cpp",
                ".c",
                ".go",
                ".rs",
            ]
            if any(
                ext in metadata.get("file_path", "").lower()
                for ext in important_extensions
            ):
                priority += 20

            # é…ç½®æ–‡ä»¶å’Œé‡è¦æ–‡ä»¶
            important_files = ["readme", "config", "setup", "main", "index", "__init__"]
            if any(
                name in metadata.get("file_path", "").lower()
                for name in important_files
            ):
                priority += 15

            # ä»£ç ç»“æ„å…³é”®è¯
            code_keywords = [
                "class ",
                "function ",
                "def ",
                "import ",
                "from ",
                "export",
            ]
            priority += sum(3 for keyword in code_keywords if keyword in content)

            # é¿å…æµ‹è¯•æ–‡ä»¶è¿‡å¤š
            if "test" in metadata.get("file_path", "").lower():
                priority -= 5

            # å†…å®¹é•¿åº¦é€‚ä¸­ä¼˜å…ˆ
            content_len = len(doc.page_content)
            if 200 <= content_len <= 2000:
                priority += 10
            elif content_len > 5000:
                priority -= 5

            return priority

        # 3. æ’åºå¹¶é™åˆ¶æ•°é‡
        filtered_docs.sort(key=get_document_priority, reverse=True)

        if len(filtered_docs) > self.max_documents_per_version:
            filtered_docs = filtered_docs[: self.max_documents_per_version]
            self.logger.info(
                f"{version_name}: æ•°é‡é™åˆ¶ {after_filter_count} -> {len(filtered_docs)} ä¸ªæ–‡æ¡£"
            )

        self.stats["filtered_documents"] += original_count - len(filtered_docs)
        return filtered_docs

    def build_vector_db_in_batches(
        self, documents: List, rag_system, version_name: str
    ):
        """
        æ‰¹é‡æ„å»ºå‘é‡æ•°æ®åº“

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            rag_system: RAGç³»ç»Ÿå®ä¾‹
            version_name: ç‰ˆæœ¬åç§°
        """
        total_docs = len(documents)
        if total_docs == 0:
            self.logger.warning(f"{version_name}: æ²¡æœ‰æ–‡æ¡£éœ€è¦ç´¢å¼•")
            return

        self.logger.info(
            f"{version_name}: å¼€å§‹æ‰¹é‡ç´¢å¼• {total_docs} ä¸ªæ–‡æ¡£ï¼Œæ‰¹å¤§å°: {self.batch_size}"
        )

        # åˆ†æ‰¹å¤„ç†
        for i in range(0, total_docs, self.batch_size):
            batch = documents[i : i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (total_docs + self.batch_size - 1) // self.batch_size

            self.logger.info(
                f"{version_name}: å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} ä¸ªæ–‡æ¡£)"
            )

            try:
                if i == 0:
                    # ç¬¬ä¸€æ‰¹ï¼šåˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨
                    rag_system.vectorstore = (
                        rag_system._create_vectorstore_from_documents(batch)
                    )
                else:
                    # åç»­æ‰¹æ¬¡ï¼šæ·»åŠ åˆ°ç°æœ‰å‘é‡å­˜å‚¨
                    rag_system._add_documents_to_vectorstore(batch)

            except Exception as e:
                self.logger.error(f"{version_name}: æ‰¹æ¬¡ {batch_num} ç´¢å¼•å¤±è´¥: {e}")
                raise

    def check_file_size(self, file_path: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦åˆé€‚å¤„ç†"""
        try:
            file_size_mb = Path(file_path).stat().st_size / (1024 * 1024)
            if self.skip_large_files and file_size_mb > self.max_file_size_mb:
                self.logger.warning(f"è·³è¿‡å¤§æ–‡ä»¶ {file_path}: {file_size_mb:.1f}MB")
                return False
            return True
        except Exception:
            return True  # å¦‚æœæ— æ³•è·å–æ–‡ä»¶å¤§å°ï¼Œç»§ç»­å¤„ç†

    def find_git_enhanced_files(self) -> List[Dict[str, str]]:
        """æŸ¥æ‰¾æ‰€æœ‰çš„Gitå¢å¼ºç‰ˆmarkdownæ–‡ä»¶"""
        git_files = []

        if not self.repomix_dir.exists():
            self.logger.error(f"repomixç›®å½•ä¸å­˜åœ¨: {self.repomix_dir}")
            return git_files

        for repo_dir in self.repomix_dir.iterdir():
            if not repo_dir.is_dir() or not repo_dir.name.startswith("repository-"):
                continue

            # æå–ä»“åº“å
            repo_name = repo_dir.name.replace("repository-", "")

            # æŸ¥æ‰¾Gitå¢å¼ºç‰ˆæ–‡ä»¶
            git_file_pattern = f"repomix-output-{repo_name}-with-git.md"
            git_file_path = repo_dir / git_file_pattern

            if git_file_path.exists():
                # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œç»™å‡ºé¢„ä¼°æ—¶é—´
                file_size_mb = git_file_path.stat().st_size / (1024 * 1024)

                git_files.append(
                    {
                        "repo_name": repo_name,
                        "repo_dir": str(repo_dir),
                        "git_file": str(git_file_path),
                        "file_size_mb": file_size_mb,
                        "output_dir": str(self.output_dir / f"repository-{repo_name}"),
                    }
                )
                self.logger.info(f"æ‰¾åˆ°Gitå¢å¼ºæ–‡ä»¶: {repo_name} ({file_size_mb:.1f}MB)")
            else:
                self.logger.warning(f"è·³è¿‡ä»“åº“ {repo_name}: æœªæ‰¾åˆ°Gitå¢å¼ºç‰ˆæ–‡ä»¶")
                self.stats["skipped_repos"] += 1

        self.stats["total_repos"] = len(git_files) + self.stats["skipped_repos"]
        self.logger.info(f"æ€»å…±æ‰¾åˆ° {len(git_files)} ä¸ªGitå¢å¼ºç‰ˆæ–‡ä»¶")

        # æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼Œå…ˆå¤„ç†å°æ–‡ä»¶
        git_files.sort(key=lambda x: x["file_size_mb"])
        return git_files

    def build_single_repository(self, repo_info: Dict[str, str]) -> bool:
        """ä¸ºå•ä¸ªä»“åº“æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        repo_name = repo_info["repo_name"]
        git_file = repo_info["git_file"]
        output_dir = repo_info["output_dir"]
        file_size_mb = repo_info.get("file_size_mb", 0)

        self.logger.info(f"å¼€å§‹å¤„ç†ä»“åº“: {repo_name} ({file_size_mb:.1f}MB)")

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        if not self.check_file_size(git_file):
            self.logger.warning(f"è·³è¿‡ä»“åº“ {repo_name}: æ–‡ä»¶è¿‡å¤§")
            self.stats["skipped_repos"] += 1
            return False

        try:
            # æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(output_dir):
                self.logger.info(f"æ¸…ç†å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“: {repo_name}")
                shutil.rmtree(output_dir)

            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(output_dir, exist_ok=True)

            # è¯»å–markdownæ–‡ä»¶
            with open(git_file, "r", encoding="utf-8") as f:
                md_content = f.read()

            # è§£æä»£ç å—
            start_time = datetime.now()
            code_blocks = self.parser.parse_markdown(md_content)
            parse_time = (datetime.now() - start_time).total_seconds()

            if not code_blocks:
                self.logger.warning(f"ä»“åº“ {repo_name}: æœªè§£æåˆ°ä»»ä½•ä»£ç å—")
                return False

            self.logger.info(
                f"ä»“åº“ {repo_name}: è§£æå‡º {len(code_blocks)} ä¸ªä»£ç å— (è€—æ—¶ {parse_time:.1f}s)"
            )

            # åˆ›å»ºä¸‰ç§ä¸åŒç‰ˆæœ¬çš„æ–‡æ¡£
            basic_docs = self.processor.create_basic_documents(code_blocks)
            enhanced_docs = self.processor.create_enhanced_documents(code_blocks)
            compressed_docs = self.processor.create_compressed_documents(code_blocks)

            self.logger.info(
                f"ä»“åº“ {repo_name}: åŸå§‹æ–‡æ¡£æ•° - åŸºç¡€ç‰ˆ:{len(basic_docs)}, å¢å¼ºç‰ˆ:{len(enhanced_docs)}, å‹ç¼©ç‰ˆ:{len(compressed_docs)}"
            )

            # è¿‡æ»¤å’Œä¼˜åŒ–æ–‡æ¡£
            filtered_basic = self.filter_and_prioritize_documents(
                basic_docs, f"{repo_name}_basic"
            )
            filtered_enhanced = self.filter_and_prioritize_documents(
                enhanced_docs, f"{repo_name}_enhanced"
            )
            filtered_compressed = self.filter_and_prioritize_documents(
                compressed_docs, f"{repo_name}_compressed"
            )

            # ä¸ºæ¯ç§ç‰ˆæœ¬åˆ›å»ºå‘é‡æ•°æ®åº“
            versions = [
                ("basic", filtered_basic),
                ("enhanced", filtered_enhanced),
                ("compressed", filtered_compressed),
            ]

            index_start_time = datetime.now()

            for version_name, documents in versions:
                if not documents:
                    self.logger.warning(
                        f"ä»“åº“ {repo_name} {version_name}ç‰ˆæœ¬: è¿‡æ»¤åæ²¡æœ‰æ–‡æ¡£éœ€è¦ç´¢å¼•"
                    )
                    continue

                # åˆ›å»ºRAGç³»ç»Ÿ
                collection_name = f"{repo_name}_{version_name}"
                persist_dir = os.path.join(output_dir, f"chroma_db_{version_name}")

                rag_system = LangChainRAGSystem(collection_name)
                rag_system.persist_directory = persist_dir

                # æ‰¹é‡ç´¢å¼•æ–‡æ¡£
                version_start_time = datetime.now()
                self.build_vector_db_in_batches(
                    documents, rag_system, f"{repo_name}_{version_name}"
                )
                version_time = (datetime.now() - version_start_time).total_seconds()

                self.logger.info(
                    f"ä»“åº“ {repo_name} {version_name}ç‰ˆæœ¬: å·²ç´¢å¼• {len(documents)} ä¸ªæ–‡æ¡£ (è€—æ—¶ {version_time:.1f}s)"
                )

            total_index_time = (datetime.now() - index_start_time).total_seconds()

            # ä¿å­˜ä»“åº“å…ƒæ•°æ®
            metadata = {
                "repo_name": repo_name,
                "source_file": git_file,
                "file_size_mb": file_size_mb,
                "build_time": datetime.now().isoformat(),
                "processing_time": {
                    "parse_time_seconds": parse_time,
                    "index_time_seconds": total_index_time,
                    "total_time_seconds": parse_time + total_index_time,
                },
                "code_blocks_count": len(code_blocks),
                "original_documents_count": {
                    "basic": len(basic_docs),
                    "enhanced": len(enhanced_docs),
                    "compressed": len(compressed_docs),
                },
                "filtered_documents_count": {
                    "basic": len(filtered_basic),
                    "enhanced": len(filtered_enhanced),
                    "compressed": len(filtered_compressed),
                },
                "optimization_settings": {
                    "max_documents_per_version": self.max_documents_per_version,
                    "min_content_length": self.min_content_length,
                    "batch_size": self.batch_size,
                    "max_file_size_mb": self.max_file_size_mb,
                },
                "total_commits": sum(len(block.git_commits) for block in code_blocks),
                "files_processed": [block.file_path for block in code_blocks],
            }

            metadata_file = os.path.join(output_dir, "metadata.json")
            with open(metadata_file, "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            self.stats["processed_repos"] += 1
            self.stats["total_documents"] += (
                len(filtered_basic) + len(filtered_enhanced) + len(filtered_compressed)
            )

            self.logger.info(
                f"âœ… ä»“åº“ {repo_name} å¤„ç†å®Œæˆ (æ€»è€—æ—¶ {parse_time + total_index_time:.1f}s)"
            )
            return True

        except Exception as e:
            self.logger.error(f"âŒ ä»“åº“ {repo_name} å¤„ç†å¤±è´¥: {e}")
            self.stats["failed_repos"] += 1
            self.stats["processing_errors"].append(
                {
                    "repo_name": repo_name,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat(),
                }
            )
            return False

    def build_all_repositories(self) -> Dict:
        """æ‰¹é‡æ„å»ºæ‰€æœ‰ä»“åº“çš„å‘é‡æ•°æ®åº“"""
        self.logger.info("å¼€å§‹æ‰¹é‡æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆä¼˜åŒ–ç‰ˆï¼‰...")
        self.logger.info(
            f"ä¼˜åŒ–è®¾ç½®: æœ€å¤§æ–‡æ¡£æ•°={self.max_documents_per_version}, æ‰¹å¤§å°={self.batch_size}"
        )

        # æŸ¥æ‰¾æ‰€æœ‰Gitå¢å¼ºç‰ˆæ–‡ä»¶
        git_files = self.find_git_enhanced_files()

        if not git_files:
            self.logger.error("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•Gitå¢å¼ºç‰ˆæ–‡ä»¶")
            return self.stats

        # ä¼°ç®—æ€»å¤„ç†æ—¶é—´
        total_size_mb = sum(f.get("file_size_mb", 0) for f in git_files)
        self.logger.info(f"é¢„è®¡å¤„ç† {total_size_mb:.1f}MB æ•°æ®")

        # å¤„ç†æ¯ä¸ªä»“åº“
        start_time = datetime.now()
        for i, repo_info in enumerate(git_files, 1):
            self.logger.info(
                f"è¿›åº¦: {i}/{len(git_files)} - å¤„ç†ä»“åº“ {repo_info['repo_name']}"
            )

            if self.build_single_repository(repo_info):
                # è®¡ç®—å¹³å‡é€Ÿåº¦å’Œå‰©ä½™æ—¶é—´
                elapsed_time = (datetime.now() - start_time).total_seconds()
                avg_time_per_repo = elapsed_time / i
                remaining_repos = len(git_files) - i
                estimated_remaining_time = avg_time_per_repo * remaining_repos

                self.logger.info(
                    f"å¹³å‡å¤„ç†æ—¶é—´: {avg_time_per_repo:.1f}s/ä»“åº“, é¢„è®¡å‰©ä½™æ—¶é—´: {estimated_remaining_time / 60:.1f}åˆ†é’Ÿ"
                )

        total_time = (datetime.now() - start_time).total_seconds()

        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.logger.info("=" * 60)
        self.logger.info("æ„å»ºå®Œæˆ! ç»Ÿè®¡æŠ¥å‘Š:")
        self.logger.info(f"æ€»ä»“åº“æ•°: {self.stats['total_repos']}")
        self.logger.info(f"æˆåŠŸå¤„ç†: {self.stats['processed_repos']}")
        self.logger.info(f"è·³è¿‡ä»“åº“: {self.stats['skipped_repos']}")
        self.logger.info(f"å¤±è´¥ä»“åº“: {self.stats['failed_repos']}")
        self.logger.info(f"æ€»æ–‡æ¡£æ•°: {self.stats['total_documents']}")
        self.logger.info(f"è¿‡æ»¤æ–‡æ¡£æ•°: {self.stats['filtered_documents']}")
        self.logger.info(f"æ€»å¤„ç†æ—¶é—´: {total_time / 60:.1f}åˆ†é’Ÿ")
        self.logger.info(
            f"å¹³å‡å¤„ç†é€Ÿåº¦: {total_time / max(1, self.stats['processed_repos']):.1f}s/ä»“åº“"
        )

        if self.stats["processing_errors"]:
            self.logger.info("\nå¤±è´¥çš„ä»“åº“:")
            for error in self.stats["processing_errors"]:
                self.logger.info(f"  - {error['repo_name']}: {error['error']}")

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        self.stats["total_processing_time_seconds"] = total_time
        stats_file = self.output_dir / "build_statistics.json"
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)

        self.logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
        return self.stats

    def verify_builds(self) -> Dict[str, bool]:
        """éªŒè¯æ„å»ºç»“æœ"""
        self.logger.info("å¼€å§‹éªŒè¯æ„å»ºç»“æœ...")

        verification_results = {}

        for repo_dir in self.output_dir.iterdir():
            if not repo_dir.is_dir() or not repo_dir.name.startswith("repository-"):
                continue

            repo_name = repo_dir.name.replace("repository-", "")

            # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
            required_files = [
                "metadata.json",
                "chroma_db_basic",
                "chroma_db_enhanced",
                "chroma_db_compressed",
            ]

            all_exists = True
            for req_file in required_files:
                file_path = repo_dir / req_file
                if not file_path.exists():
                    self.logger.warning(f"ä»“åº“ {repo_name}: ç¼ºå°‘ {req_file}")
                    all_exists = False

            verification_results[repo_name] = all_exists

            if all_exists:
                self.logger.info(f"âœ… ä»“åº“ {repo_name}: éªŒè¯é€šè¿‡")
            else:
                self.logger.error(f"âŒ ä»“åº“ {repo_name}: éªŒè¯å¤±è´¥")

        # ä¿å­˜éªŒè¯ç»“æœ
        verification_file = self.output_dir / "verification_results.json"
        with open(verification_file, "w", encoding="utf-8") as f:
            json.dump(verification_results, f, ensure_ascii=False, indent=2)

        passed = sum(verification_results.values())
        total = len(verification_results)
        self.logger.info(f"éªŒè¯å®Œæˆ: {passed}/{total} ä¸ªä»“åº“é€šè¿‡éªŒè¯")

        return verification_results


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ ä¼˜åŒ–ç‰ˆæ‰¹é‡å‘é‡æ•°æ®åº“æ„å»ºå·¥å…·")
    print("=" * 60)

    # æ£€æŸ¥ä¾èµ–
    try:
        import importlib.util

        spec = importlib.util.find_spec("rag_system")
        if spec is None:
            raise ImportError("rag_system module not found")
        print("âœ… ä¾èµ–æ£€æŸ¥é€šè¿‡")
    except ImportError as e:
        print(f"âŒ ä¾èµ–æ£€æŸ¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿rag_system.pyåœ¨åŒä¸€ç›®å½•ä¸‹ï¼Œå¹¶å®‰è£…äº†æ‰€éœ€ä¾èµ–")
        return

    # åˆ›å»ºæ„å»ºå™¨ï¼ˆä¼˜åŒ–å‚æ•°ï¼‰
    builder = OptimizedVectorDatabaseBuilder(
        max_documents_per_version=2000,  # ä»18ké™åˆ°2k
        min_content_length=100,
        batch_size=100,
        skip_large_files=True,
        max_file_size_mb=5.0,
    )

    print(f"ğŸ”§ ä¼˜åŒ–è®¾ç½®:")
    print(f"   æœ€å¤§æ–‡æ¡£æ•°/ç‰ˆæœ¬: {builder.max_documents_per_version}")
    print(f"   æœ€å°å†…å®¹é•¿åº¦: {builder.min_content_length}")
    print(f"   æ‰¹å¤„ç†å¤§å°: {builder.batch_size}")
    print(f"   æœ€å¤§æ–‡ä»¶å¤§å°: {builder.max_file_size_mb}MB")
    print()

    # æ‰§è¡Œæ„å»º
    stats = builder.build_all_repositories()

    # éªŒè¯ç»“æœ
    verification = builder.verify_builds()

    print("\n" + "=" * 60)
    print("ğŸ‰ æ„å»ºå®Œæˆ!")
    print(
        f"ğŸ“Š æˆåŠŸ: {stats['processed_repos']}, å¤±è´¥: {stats['failed_repos']}, è·³è¿‡: {stats['skipped_repos']}"
    )
    print(f"ğŸ“„ æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
    print(f"ğŸ—‚ï¸ è¿‡æ»¤æ–‡æ¡£æ•°: {stats['filtered_documents']}")
    print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {stats.get('total_processing_time_seconds', 0) / 60:.1f}åˆ†é’Ÿ")

    passed_verification = sum(verification.values())
    total_verification = len(verification)
    print(f"âœ… éªŒè¯é€šè¿‡: {passed_verification}/{total_verification}")

    print("\nğŸ“ è¾“å‡ºç›®å½•ç»“æ„:")
    output_dir = Path("./vector_dbs_optimized")
    if output_dir.exists():
        for item in sorted(output_dir.iterdir()):
            if item.is_dir() and item.name.startswith("repository-"):
                print(f"  ğŸ“‚ {item.name}/")
                for subitem in sorted(item.iterdir()):
                    if subitem.is_dir():
                        print(f"    ğŸ“Š {subitem.name}/")
                    else:
                        print(f"    ğŸ“„ {subitem.name}")

    print("\nğŸ“ æ—¥å¿—æ–‡ä»¶: optimized_vector_db_build.log")
    print("ğŸ“ˆ ç»Ÿè®¡æ–‡ä»¶: ./vector_dbs_optimized/build_statistics.json")
    print("ğŸ” éªŒè¯æ–‡ä»¶: ./vector_dbs_optimized/verification_results.json")


if __name__ == "__main__":
    main()
