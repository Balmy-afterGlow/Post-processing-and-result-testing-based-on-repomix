#!/usr/bin/env python3
"""
æ‰¹é‡å‘é‡æ•°æ®åº“æ„å»ºè„šæœ¬
åŸºäºrepomix_mdç›®å½•ä¸‹çš„Gitå¢å¼ºç‰ˆmarkdownæ–‡æ¡£ï¼Œä¸ºæ¯ä¸ªä»“åº“æ„å»ºå‘é‡æ•°æ®åº“
"""

import os
import json
import shutil
from pathlib import Path
from typing import List, Dict
from datetime import datetime
import logging

# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œé¿å…ç½‘ç»œè¯·æ±‚
os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_DATASETS_OFFLINE"] = "1"

# ä»åŒç›®å½•å¯¼å…¥RAGç³»ç»Ÿ
from rag_system import MarkdownParser, DocumentProcessor, LangChainRAGSystem


class VectorDatabaseBuilder:
    """å‘é‡æ•°æ®åº“æ‰¹é‡æ„å»ºå™¨"""

    def __init__(
        self, repomix_dir: str = "../repomix_md", output_dir: str = "./vector_dbs"
    ):
        """
        åˆå§‹åŒ–æ„å»ºå™¨

        Args:
            repomix_dir: repomixè¾“å‡ºç›®å½•
            output_dir: å‘é‡æ•°æ®åº“è¾“å‡ºç›®å½•
        """
        self.repomix_dir = Path(repomix_dir)
        self.output_dir = Path(output_dir)
        self.parser = MarkdownParser()
        self.processor = DocumentProcessor()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(exist_ok=True)

        # è®¾ç½®æ—¥å¿—
        self._setup_logging()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_repos": 0,
            "processed_repos": 0,
            "skipped_repos": 0,
            "failed_repos": 0,
            "total_documents": 0,
            "processing_errors": [],
        }

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler("vector_db_build.log"),
                logging.StreamHandler(),
            ],
        )
        self.logger = logging.getLogger(__name__)

    def find_git_enhanced_files(self) -> List[Dict[str, str]]:
        """æŸ¥æ‰¾æ‰€æœ‰çš„Gitå¢å¼ºç‰ˆmarkdownæ–‡ä»¶"""
        git_files = []

        if not self.repomix_dir.exists():
            self.logger.error(f"repomixç›®å½•ä¸å­˜åœ¨: {self.repomix_dir}")
            return git_files

        for repo_dir in self.repomix_dir.iterdir():
            if not repo_dir.is_dir() or not repo_dir.name.startswith("repository-"):
                continue

            # æå–ä»“åº“å
            repo_name = repo_dir.name.replace("repository-", "")

            # æŸ¥æ‰¾Gitå¢å¼ºç‰ˆæ–‡ä»¶
            git_file_pattern = f"repomix-output-{repo_name}-with-git.md"
            git_file_path = repo_dir / git_file_pattern

            if git_file_path.exists():
                git_files.append(
                    {
                        "repo_name": repo_name,
                        "repo_dir": str(repo_dir),
                        "git_file": str(git_file_path),
                        "output_dir": str(self.output_dir / f"repository-{repo_name}"),
                    }
                )
                self.logger.info(f"æ‰¾åˆ°Gitå¢å¼ºæ–‡ä»¶: {repo_name}")
            else:
                self.logger.warning(f"è·³è¿‡ä»“åº“ {repo_name}: æœªæ‰¾åˆ°Gitå¢å¼ºç‰ˆæ–‡ä»¶")
                self.stats["skipped_repos"] += 1

        self.stats["total_repos"] = len(git_files) + self.stats["skipped_repos"]
        self.logger.info(f"æ€»å…±æ‰¾åˆ° {len(git_files)} ä¸ªGitå¢å¼ºç‰ˆæ–‡ä»¶")
        return git_files

    def build_single_repository(self, repo_info: Dict[str, str]) -> bool:
        """ä¸ºå•ä¸ªä»“åº“æ„å»ºå‘é‡æ•°æ®åº“"""
        repo_name = repo_info["repo_name"]
        git_file = repo_info["git_file"]
        output_dir = repo_info["output_dir"]

        self.logger.info(f"å¼€å§‹å¤„ç†ä»“åº“: {repo_name}")

        try:
            # æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(output_dir):
                self.logger.info(f"æ¸…ç†å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“: {repo_name}")
                shutil.rmtree(output_dir)

            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(output_dir, exist_ok=True)

            # è¯»å–markdownæ–‡ä»¶
            with open(git_file, "r", encoding="utf-8") as f:
                md_content = f.read()

            # è§£æä»£ç å—
            code_blocks = self.parser.parse_markdown(md_content)

            if not code_blocks:
                self.logger.warning(f"ä»“åº“ {repo_name}: æœªè§£æåˆ°ä»»ä½•ä»£ç å—")
                return False

            self.logger.info(f"ä»“åº“ {repo_name}: è§£æå‡º {len(code_blocks)} ä¸ªä»£ç å—")

            # åˆ›å»ºä¸‰ç§ä¸åŒç‰ˆæœ¬çš„æ–‡æ¡£
            basic_docs = self.processor.create_basic_documents(code_blocks)
            enhanced_docs = self.processor.create_enhanced_documents(code_blocks)
            compressed_docs = self.processor.create_compressed_documents(code_blocks)

            self.logger.info(
                f"ä»“åº“ {repo_name}: åˆ›å»ºæ–‡æ¡£ - åŸºç¡€ç‰ˆ:{len(basic_docs)}, å¢å¼ºç‰ˆ:{len(enhanced_docs)}, å‹ç¼©ç‰ˆ:{len(compressed_docs)}"
            )

            # ä¸ºæ¯ç§ç‰ˆæœ¬åˆ›å»ºå‘é‡æ•°æ®åº“
            versions = [
                ("basic", basic_docs),
                ("enhanced", enhanced_docs),
                ("compressed", compressed_docs),
            ]

            for version_name, documents in versions:
                if not documents:
                    self.logger.warning(
                        f"ä»“åº“ {repo_name} {version_name}ç‰ˆæœ¬: æ²¡æœ‰æ–‡æ¡£éœ€è¦ç´¢å¼•"
                    )
                    continue

                # åˆ›å»ºRAGç³»ç»Ÿ
                collection_name = f"{repo_name}_{version_name}"
                persist_dir = os.path.join(output_dir, f"chroma_db_{version_name}")

                rag_system = LangChainRAGSystem(collection_name)
                rag_system.persist_directory = persist_dir

                # ç´¢å¼•æ–‡æ¡£
                rag_system.index_documents(documents)
                self.logger.info(
                    f"ä»“åº“ {repo_name} {version_name}ç‰ˆæœ¬: å·²ç´¢å¼• {len(documents)} ä¸ªæ–‡æ¡£"
                )

            # ä¿å­˜ä»“åº“å…ƒæ•°æ®
            metadata = {
                "repo_name": repo_name,
                "source_file": git_file,
                "build_time": datetime.now().isoformat(),
                "code_blocks_count": len(code_blocks),
                "documents_count": {
                    "basic": len(basic_docs),
                    "enhanced": len(enhanced_docs),
                    "compressed": len(compressed_docs),
                },
                "total_commits": sum(len(block.git_commits) for block in code_blocks),
                "files_processed": [block.file_path for block in code_blocks],
            }

            metadata_file = os.path.join(output_dir, "metadata.json")
            with open(metadata_file, "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            self.stats["processed_repos"] += 1
            self.stats["total_documents"] += (
                len(basic_docs) + len(enhanced_docs) + len(compressed_docs)
            )

            self.logger.info(f"âœ… ä»“åº“ {repo_name} å¤„ç†å®Œæˆ")
            return True

        except Exception as e:
            self.logger.error(f"âŒ ä»“åº“ {repo_name} å¤„ç†å¤±è´¥: {e}")
            self.stats["failed_repos"] += 1
            self.stats["processing_errors"].append(
                {
                    "repo_name": repo_name,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat(),
                }
            )
            return False

    def build_all_repositories(self) -> Dict:
        """æ‰¹é‡æ„å»ºæ‰€æœ‰ä»“åº“çš„å‘é‡æ•°æ®åº“"""
        self.logger.info("å¼€å§‹æ‰¹é‡æ„å»ºå‘é‡æ•°æ®åº“...")

        # æŸ¥æ‰¾æ‰€æœ‰Gitå¢å¼ºç‰ˆæ–‡ä»¶
        git_files = self.find_git_enhanced_files()

        if not git_files:
            self.logger.error("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•Gitå¢å¼ºç‰ˆæ–‡ä»¶")
            return self.stats

        # å¤„ç†æ¯ä¸ªä»“åº“
        success_count = 0
        for i, repo_info in enumerate(git_files, 1):
            self.logger.info(
                f"è¿›åº¦: {i}/{len(git_files)} - å¤„ç†ä»“åº“ {repo_info['repo_name']}"
            )

            if self.build_single_repository(repo_info):
                success_count += 1

        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.logger.info("=" * 60)
        self.logger.info("æ„å»ºå®Œæˆ! ç»Ÿè®¡æŠ¥å‘Š:")
        self.logger.info(f"æ€»ä»“åº“æ•°: {self.stats['total_repos']}")
        self.logger.info(f"æˆåŠŸå¤„ç†: {self.stats['processed_repos']}")
        self.logger.info(f"è·³è¿‡ä»“åº“: {self.stats['skipped_repos']}")
        self.logger.info(f"å¤±è´¥ä»“åº“: {self.stats['failed_repos']}")
        self.logger.info(f"æ€»æ–‡æ¡£æ•°: {self.stats['total_documents']}")

        if self.stats["processing_errors"]:
            self.logger.info("\nå¤±è´¥çš„ä»“åº“:")
            for error in self.stats["processing_errors"]:
                self.logger.info(f"  - {error['repo_name']}: {error['error']}")

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = self.output_dir / "build_statistics.json"
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)

        self.logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
        return self.stats

    def verify_builds(self) -> Dict[str, bool]:
        """éªŒè¯æ„å»ºç»“æœ"""
        self.logger.info("å¼€å§‹éªŒè¯æ„å»ºç»“æœ...")

        verification_results = {}

        for repo_dir in self.output_dir.iterdir():
            if not repo_dir.is_dir() or not repo_dir.name.startswith("repository-"):
                continue

            repo_name = repo_dir.name.replace("repository-", "")

            # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
            required_files = [
                "metadata.json",
                "chroma_db_basic",
                "chroma_db_enhanced",
                "chroma_db_compressed",
            ]

            all_exists = True
            for req_file in required_files:
                file_path = repo_dir / req_file
                if not file_path.exists():
                    self.logger.warning(f"ä»“åº“ {repo_name}: ç¼ºå°‘ {req_file}")
                    all_exists = False

            verification_results[repo_name] = all_exists

            if all_exists:
                self.logger.info(f"âœ… ä»“åº“ {repo_name}: éªŒè¯é€šè¿‡")
            else:
                self.logger.error(f"âŒ ä»“åº“ {repo_name}: éªŒè¯å¤±è´¥")

        # ä¿å­˜éªŒè¯ç»“æœ
        verification_file = self.output_dir / "verification_results.json"
        with open(verification_file, "w", encoding="utf-8") as f:
            json.dump(verification_results, f, ensure_ascii=False, indent=2)

        passed = sum(verification_results.values())
        total = len(verification_results)
        self.logger.info(f"éªŒè¯å®Œæˆ: {passed}/{total} ä¸ªä»“åº“é€šè¿‡éªŒè¯")

        return verification_results


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ æ‰¹é‡å‘é‡æ•°æ®åº“æ„å»ºå·¥å…·")
    print("=" * 60)

    # æ£€æŸ¥ä¾èµ–
    try:
        import importlib.util

        spec = importlib.util.find_spec("rag_system")
        if spec is None:
            raise ImportError("rag_system module not found")
        print("âœ… ä¾èµ–æ£€æŸ¥é€šè¿‡")
    except ImportError as e:
        print(f"âŒ ä¾èµ–æ£€æŸ¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿rag_system.pyåœ¨åŒä¸€ç›®å½•ä¸‹ï¼Œå¹¶å®‰è£…äº†æ‰€éœ€ä¾èµ–")
        return

    # åˆ›å»ºæ„å»ºå™¨
    builder = VectorDatabaseBuilder()

    # æ‰§è¡Œæ„å»º
    stats = builder.build_all_repositories()

    # éªŒè¯ç»“æœ
    verification = builder.verify_builds()

    print("\n" + "=" * 60)
    print("ğŸ‰ æ„å»ºå®Œæˆ!")
    print(
        f"ğŸ“Š æˆåŠŸ: {stats['processed_repos']}, å¤±è´¥: {stats['failed_repos']}, è·³è¿‡: {stats['skipped_repos']}"
    )
    print(f"ğŸ“„ æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")

    passed_verification = sum(verification.values())
    total_verification = len(verification)
    print(f"âœ… éªŒè¯é€šè¿‡: {passed_verification}/{total_verification}")

    print("\nğŸ“ è¾“å‡ºç›®å½•ç»“æ„:")
    output_dir = Path("./vector_dbs")
    if output_dir.exists():
        for item in sorted(output_dir.iterdir()):
            if item.is_dir() and item.name.startswith("repository-"):
                print(f"  ğŸ“‚ {item.name}/")
                for subitem in sorted(item.iterdir()):
                    if subitem.is_dir():
                        print(f"    ğŸ“Š {subitem.name}/")
                    else:
                        print(f"    ğŸ“„ {subitem.name}")

    print("\nğŸ“ æ—¥å¿—æ–‡ä»¶: vector_db_build.log")
    print("ğŸ“ˆ ç»Ÿè®¡æ–‡ä»¶: ./vector_dbs/build_statistics.json")
    print("ğŸ” éªŒè¯æ–‡ä»¶: ./vector_dbs/verification_results.json")


if __name__ == "__main__":
    main()
