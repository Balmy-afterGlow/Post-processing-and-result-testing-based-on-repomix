#!/usr/bin/env python3
"""
RAGè¯„ä¼°ç³»ç»Ÿ - è‡ªåŠ¨å¯¹AIè¾“å‡ºä¸æ ‡å‡†ç­”æ¡ˆè¿›è¡Œå¤šç»´åº¦è¯„ä¼°
æ”¯æŒå¤šç§æŒ‡æ ‡ï¼šF1ã€Top-Kã€MAPã€ç»“æ„å®Œæ•´æ€§ã€ç½®ä¿¡åº¦ã€AIåˆ†ææ­£ç¡®æ€§ç­‰
"""

import os
import sys
import json
import argparse
import logging
from dotenv import load_dotenv
from pathlib import Path
from typing import Dict, List, Any, Optional
from collections import defaultdict
import re

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# æ ‡å‡†åº“å¯¼å…¥
from difflib import SequenceMatcher
import numpy as np

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥ï¼ˆéœ€è¦æ—¶å®‰è£…ï¼‰
try:
    import openai

    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print(
        "Warning: openai package not available. AI similarity analysis will be disabled."
    )

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print(
        "Warning: scikit-learn not available. Advanced text similarity will use basic methods."
    )

# æœ¬åœ°æ¨¡å—å¯¼å…¥
from configs.prompts import get_evaluation_prompt


load_dotenv()


class RAGEvaluator:
    """RAGè¯„ä¼°å™¨ç±»"""

    def __init__(
        self,
        api_key: Optional[str] = None,
        openai_base_url: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            api_key: OpenAI APIå¯†é’¥ï¼Œç”¨äºAIåˆ†ææ­£ç¡®æ€§è¯„ä¼°
            openai_base_url: OpenAI APIåŸºç¡€URLï¼Œæ”¯æŒè‡ªå®šä¹‰æ¥å£
        """
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        self.openai_base_url = openai_base_url or os.getenv(
            "OPENAI_BASE_URL", "https://api.deepseek.com"
        )

        # è®¾ç½®æ—¥å¿—
        self.setup_logging()

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        if OPENAI_AVAILABLE and self.api_key:
            try:
                self.client = openai.OpenAI(
                    api_key=self.api_key,
                    base_url=self.openai_base_url,
                    timeout=30.0,  # è®¾ç½®è¶…æ—¶æ—¶é—´
                )
                self.ai_analysis_enabled = True
                self.logger.info(
                    f"AI analysis enabled with base URL: {self.openai_base_url}"
                )
            except Exception as e:
                self.logger.error(f"Failed to initialize OpenAI client: {e}")
                self.ai_analysis_enabled = False
        else:
            self.ai_analysis_enabled = False
            if not OPENAI_AVAILABLE:
                self.logger.warning("OpenAI package not available")
            if not self.api_key:
                self.logger.warning("No API key provided")

        # åˆå§‹åŒ–TF-IDFå‘é‡åŒ–å™¨
        if SKLEARN_AVAILABLE:
            self.vectorizer = TfidfVectorizer(
                stop_words="english", ngram_range=(1, 2), max_features=5000
            )

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[logging.FileHandler("evaluation.log"), logging.StreamHandler()],
        )
        self.logger = logging.getLogger(__name__)

    def load_json_file(self, file_path: str) -> Optional[Dict[str, Any]]:
        """
        åŠ è½½JSONæ–‡ä»¶

        Args:
            file_path: JSONæ–‡ä»¶è·¯å¾„

        Returns:
            JSONæ•°æ®å­—å…¸ï¼ŒåŠ è½½å¤±è´¥è¿”å›None
        """
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except (FileNotFoundError, json.JSONDecodeError) as e:
            self.logger.error(f"Failed to load {file_path}: {e}")
            return None

    def save_json_file(self, data: Dict[str, Any], file_path: str) -> bool:
        """
        ä¿å­˜JSONæ–‡ä»¶

        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            file_path: ä¿å­˜è·¯å¾„

        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            self.logger.error(f"Failed to save {file_path}: {e}")
            return False

    def calculate_text_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦

        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        if not text1 or not text2:
            return 0.0

        # ä½¿ç”¨å¤šç§æ–¹æ³•è®¡ç®—ç›¸ä¼¼åº¦ï¼Œå–å¹³å‡å€¼
        similarities = []

        # 1. å­—ç¬¦çº§åˆ«çš„ç›¸ä¼¼åº¦
        char_similarity = SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
        similarities.append(char_similarity)

        # 2. è¯çº§åˆ«çš„ç›¸ä¼¼åº¦
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        word_similarity = len(words1 & words2) / max(len(words1 | words2), 1)
        similarities.append(word_similarity)

        # 3. TF-IDFä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if SKLEARN_AVAILABLE:
            try:
                tfidf_matrix = self.vectorizer.fit_transform([text1, text2])
                cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][
                    0
                ]
                similarities.append(cosine_sim)
            except Exception:
                pass

        return np.mean(similarities)

    def calculate_list_similarity(
        self, list1: List[str], list2: List[str]
    ) -> Dict[str, float]:
        """
        è®¡ç®—åˆ—è¡¨ç›¸ä¼¼åº¦ï¼ˆç”¨äºlocationå­—æ®µï¼‰

        Args:
            list1: åˆ—è¡¨1
            list2: åˆ—è¡¨2

        Returns:
            åŒ…å«å¤šç§æŒ‡æ ‡çš„å­—å…¸
        """
        if not list1 or not list2:
            return {
                "jaccard": 0.0,
                "overlap": 0.0,
                "f1": 0.0,
                "precision": 0.0,
                "recall": 0.0,
            }

        set1 = set(list1)
        set2 = set(list2)

        intersection = len(set1 & set2)
        union = len(set1 | set2)

        # Jaccardç›¸ä¼¼åº¦
        jaccard = intersection / union if union > 0 else 0.0

        # é‡å ç³»æ•°
        overlap = (
            intersection / min(len(set1), len(set2))
            if min(len(set1), len(set2)) > 0
            else 0.0
        )

        # F1ã€ç²¾ç¡®ç‡ã€å¬å›ç‡
        precision = intersection / len(set1) if len(set1) > 0 else 0.0
        recall = intersection / len(set2) if len(set2) > 0 else 0.0
        f1 = (
            2 * precision * recall / (precision + recall)
            if (precision + recall) > 0
            else 0.0
        )

        return {
            "jaccard": jaccard,
            "overlap": overlap,
            "f1": f1,
            "precision": precision,
            "recall": recall,
        }

    def calculate_top_k_accuracy(
        self, predicted: List[str], actual: List[str], k: int = 3
    ) -> float:
        """
        è®¡ç®—Top-Kå‡†ç¡®ç‡

        Args:
            predicted: é¢„æµ‹åˆ—è¡¨
            actual: å®é™…åˆ—è¡¨
            k: å–å‰kä¸ª

        Returns:
            Top-Kå‡†ç¡®ç‡
        """
        if not predicted or not actual:
            return 0.0

        predicted_k = predicted[:k]
        actual_set = set(actual)

        hits = sum(1 for item in predicted_k if item in actual_set)
        return hits / min(k, len(actual_set))

    def calculate_map_score(self, predicted: List[str], actual: List[str]) -> float:
        """
        è®¡ç®—Mean Average Precision (MAP)

        Args:
            predicted: é¢„æµ‹åˆ—è¡¨
            actual: å®é™…åˆ—è¡¨

        Returns:
            MAPåˆ†æ•°
        """
        if not predicted or not actual:
            return 0.0

        actual_set = set(actual)
        if not actual_set:
            return 0.0

        score = 0.0
        num_hits = 0

        for i, item in enumerate(predicted):
            if item in actual_set:
                num_hits += 1
                precision_at_i = num_hits / (i + 1)
                score += precision_at_i

        return score / len(actual_set) if len(actual_set) > 0 else 0.0

    def calculate_structure_completeness(
        self, ai_output: Dict[str, Any], gold_standard: Dict[str, Any]
    ) -> Dict[str, float]:
        """
        è®¡ç®—ç»“æ„å®Œæ•´æ€§

        Args:
            ai_output: AIè¾“å‡º
            gold_standard: æ ‡å‡†ç­”æ¡ˆ

        Returns:
            ç»“æ„å®Œæ•´æ€§æŒ‡æ ‡
        """
        required_fields = ["reason", "location", "fix"]
        ai_fields = set(ai_output.keys())
        gold_fields = set(gold_standard.keys())

        # å­—æ®µè¦†ç›–ç‡
        field_coverage = (
            len(ai_fields & gold_fields) / len(gold_fields) if gold_fields else 0.0
        )

        # å¿…éœ€å­—æ®µå®Œæ•´æ€§
        required_completeness = len(ai_fields & set(required_fields)) / len(
            required_fields
        )

        # å­—æ®µå†…å®¹å®Œæ•´æ€§
        content_completeness = []
        for field in required_fields:
            if field in ai_output and field in gold_standard:
                ai_content = ai_output[field]
                gold_content = gold_standard[field]

                if isinstance(ai_content, str) and isinstance(gold_content, str):
                    if ai_content.strip() and gold_content.strip():
                        content_completeness.append(1.0)
                    else:
                        content_completeness.append(0.0)
                elif isinstance(ai_content, list) and isinstance(gold_content, list):
                    if ai_content and gold_content:
                        content_completeness.append(1.0)
                    else:
                        content_completeness.append(0.0)
                else:
                    content_completeness.append(0.5)  # ç±»å‹ä¸åŒ¹é…ä½†å­˜åœ¨
            else:
                content_completeness.append(0.0)

        avg_content_completeness = (
            np.mean(content_completeness) if content_completeness else 0.0
        )

        return {
            "field_coverage": field_coverage,
            "required_completeness": required_completeness,
            "content_completeness": avg_content_completeness,
            "overall_completeness": (
                field_coverage + required_completeness + avg_content_completeness
            )
            / 3,
        }

    def calculate_confidence_score(self, ai_output: Dict[str, Any]) -> float:
        """
        è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°ï¼ˆåŸºäºå†…å®¹é•¿åº¦ã€è¯¦ç»†ç¨‹åº¦ç­‰ï¼‰

        Args:
            ai_output: AIè¾“å‡º

        Returns:
            ç½®ä¿¡åº¦åˆ†æ•° (0-1)
        """
        confidence_factors = []

        # 1. å†…å®¹é•¿åº¦å› å­
        reason_length = len(ai_output.get("reason", ""))
        fix_length = len(ai_output.get("fix", ""))

        # å½’ä¸€åŒ–é•¿åº¦åˆ†æ•°ï¼ˆå‡è®¾åˆç†é•¿åº¦ä¸º100-1000å­—ç¬¦ï¼‰
        reason_score = min(reason_length / 500, 1.0) if reason_length > 0 else 0.0
        fix_score = min(fix_length / 500, 1.0) if fix_length > 0 else 0.0

        confidence_factors.extend([reason_score, fix_score])

        # 2. ä½ç½®ä¿¡æ¯å®Œæ•´æ€§
        locations = ai_output.get("location", [])
        location_score = min(len(locations) / 3, 1.0) if locations else 0.0
        confidence_factors.append(location_score)

        # 3. å†…å®¹è¯¦ç»†ç¨‹åº¦ï¼ˆåŸºäºå¥å­æ•°é‡ï¼‰
        reason_sentences = len(re.split(r"[.!?]+", ai_output.get("reason", "")))
        fix_sentences = len(re.split(r"[.!?]+", ai_output.get("fix", "")))

        detail_score = min((reason_sentences + fix_sentences) / 6, 1.0)
        confidence_factors.append(detail_score)

        return np.mean(confidence_factors) if confidence_factors else 0.0

    async def calculate_ai_similarity(
        self, ai_output: Dict[str, Any], gold_standard: Dict[str, Any]
    ) -> Optional[float]:
        """
        ä½¿ç”¨AIæ¨¡å‹è®¡ç®—è¯­ä¹‰ç›¸ä¼¼æ€§

        Args:
            ai_output: AIè¾“å‡º
            gold_standard: æ ‡å‡†ç­”æ¡ˆ

        Returns:
            AIè¯„ä¼°çš„ç›¸ä¼¼æ€§åˆ†æ•° (0-1)ï¼Œå¤±è´¥è¿”å›None
        """
        if not self.ai_analysis_enabled:
            return None

        try:
            # æ„é€ è¯„ä¼°prompt
            prompt = get_evaluation_prompt(ai_output, gold_standard)

            self.logger.debug(f"Sending request to {self.openai_base_url}")

            # è°ƒç”¨deepseek-V3æˆ–å…¶ä»–æ¨¡å‹
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert code analysis evaluator.",
                    },
                    {"role": "user", "content": prompt},
                ],
                max_tokens=500,
                temperature=0.1,
                timeout=30.0,
            )

            result_text = response.choices[0].message.content.strip()
            self.logger.debug(f"AI response: {result_text}")

            # ä»å“åº”ä¸­æå–åˆ†æ•°
            score_match = re.search(r"score[:\s]*(\d+(?:\.\d+)?)", result_text.lower())
            if score_match:
                score = float(score_match.group(1))
                # å¦‚æœåˆ†æ•°æ˜¯0-100èŒƒå›´ï¼Œè½¬æ¢ä¸º0-1
                if score > 1:
                    score = score / 100
                return min(max(score, 0.0), 1.0)

            # å°è¯•å…¶ä»–å¯èƒ½çš„åˆ†æ•°æ ¼å¼
            score_patterns = [
                r"(\d+(?:\.\d+)?)\s*/\s*100",  # "85/100"
                r"(\d+(?:\.\d+)?)%",  # "85%"
                r"(\d+(?:\.\d+)?)\s*points",  # "85 points"
            ]

            for pattern in score_patterns:
                match = re.search(pattern, result_text.lower())
                if match:
                    score = float(match.group(1))
                    if score > 1:
                        score = score / 100
                    return min(max(score, 0.0), 1.0)

            self.logger.warning(
                f"Could not extract score from AI response: {result_text}"
            )
            return None

        except openai.APIConnectionError as e:
            self.logger.error(f"API connection error: {e}")
            self.logger.info("Please check your network connection and API base URL")
            return None
        except openai.APITimeoutError as e:
            self.logger.error(f"API timeout error: {e}")
            return None
        except openai.RateLimitError as e:
            self.logger.error(f"Rate limit exceeded: {e}")
            return None
        except openai.AuthenticationError as e:
            self.logger.error(f"Authentication error: {e}")
            self.logger.info("Please check your API key")
            return None
        except Exception as e:
            self.logger.error(f"AI similarity analysis failed: {e}")
            return None

    def evaluate_single_case(
        self, ai_output: Dict[str, Any], gold_standard: Dict[str, Any], question_id: str
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªæ¡ˆä¾‹

        Args:
            ai_output: AIè¾“å‡º
            gold_standard: æ ‡å‡†ç­”æ¡ˆ
            question_id: é—®é¢˜ID

        Returns:
            è¯„ä¼°ç»“æœ
        """
        results = {
            "question_id": question_id,
            "timestamp": str(Path().resolve()),
            "metrics": {},
        }

        # 1. æ–‡æœ¬ç›¸ä¼¼åº¦æŒ‡æ ‡
        reason_similarity = self.calculate_text_similarity(
            ai_output.get("reason", ""), gold_standard.get("reason", "")
        )

        fix_similarity = self.calculate_text_similarity(
            ai_output.get("fix", ""), gold_standard.get("fix", "")
        )

        # 2. ä½ç½®åŒ¹é…æŒ‡æ ‡
        location_metrics = self.calculate_list_similarity(
            ai_output.get("location", []), gold_standard.get("location", [])
        )

        # 3. Top-Kå’ŒMAPæŒ‡æ ‡
        top_1_acc = self.calculate_top_k_accuracy(
            ai_output.get("location", []), gold_standard.get("location", []), k=1
        )

        top_3_acc = self.calculate_top_k_accuracy(
            ai_output.get("location", []), gold_standard.get("location", []), k=3
        )

        map_score = self.calculate_map_score(
            ai_output.get("location", []), gold_standard.get("location", [])
        )

        # 4. ç»“æ„å®Œæ•´æ€§
        structure_metrics = self.calculate_structure_completeness(
            ai_output, gold_standard
        )

        # 5. ç½®ä¿¡åº¦åˆ†æ•°
        confidence = self.calculate_confidence_score(ai_output)

        # 6. AIç›¸ä¼¼æ€§åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
        ai_similarity = None
        if self.ai_analysis_enabled:
            try:
                import asyncio

                ai_similarity = asyncio.run(
                    self.calculate_ai_similarity(ai_output, gold_standard)
                )
            except Exception as e:
                self.logger.warning(
                    f"AI similarity analysis failed for {question_id}: {e}"
                )

        # 7. ç»¼åˆF1åˆ†æ•°
        overall_f1 = (
            reason_similarity * 0.4
            + fix_similarity * 0.4
            + location_metrics["f1"] * 0.2
        )

        # ç»„åˆæ‰€æœ‰æŒ‡æ ‡
        results["metrics"] = {
            "text_similarity": {
                "reason_similarity": reason_similarity,
                "fix_similarity": fix_similarity,
                "average_text_similarity": (reason_similarity + fix_similarity) / 2,
            },
            "location_matching": location_metrics,
            "ranking_metrics": {
                "top_1_accuracy": top_1_acc,
                "top_3_accuracy": top_3_acc,
                "map_score": map_score,
            },
            "structure_completeness": structure_metrics,
            "confidence_score": confidence,
            "ai_similarity": {
                "enabled": self.ai_analysis_enabled,
                "score": ai_similarity,
                "status": "success"
                if ai_similarity is not None
                else "failed"
                if self.ai_analysis_enabled
                else "disabled",
            },
            "overall_scores": {
                "f1_score": overall_f1,
                "weighted_score": (
                    overall_f1 * 0.4
                    + structure_metrics["overall_completeness"] * 0.3
                    + confidence * 0.2
                    + (ai_similarity * 0.1 if ai_similarity is not None else 0.0)
                ),
            },
        }

        return results

    def evaluate_strategy(
        self, strategy_dir: str, gold_dir: str, strategy_name: str
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªç­–ç•¥ä¸‹çš„æ‰€æœ‰æ¡ˆä¾‹

        Args:
            strategy_dir: ç­–ç•¥ç›®å½•è·¯å¾„
            gold_dir: æ ‡å‡†ç­”æ¡ˆç›®å½•è·¯å¾„
            strategy_name: ç­–ç•¥åç§°

        Returns:
            ç­–ç•¥è¯„ä¼°ç»“æœ
        """
        self.logger.info(f"Evaluating strategy: {strategy_name}")

        strategy_results = {
            "strategy": strategy_name,
            "total_cases": 0,
            "successful_evaluations": 0,
            "failed_cases": [],
            "case_results": {},
            "summary_metrics": {},
        }

        # éå†ç­–ç•¥ç›®å½•ä¸‹çš„æ‰€æœ‰é—®é¢˜
        if not os.path.exists(strategy_dir):
            self.logger.error(f"Strategy directory not found: {strategy_dir}")
            return strategy_results

        question_dirs = [
            d
            for d in os.listdir(strategy_dir)
            if os.path.isdir(os.path.join(strategy_dir, d))
        ]

        strategy_results["total_cases"] = len(question_dirs)

        all_metrics = defaultdict(list)

        for question_id in question_dirs:
            try:
                # è¯»å–AIè¾“å‡º
                ai_output_path = os.path.join(
                    strategy_dir, question_id, f"{question_id}.json"
                )
                ai_output = self.load_json_file(ai_output_path)

                if not ai_output:
                    strategy_results["failed_cases"].append(
                        {
                            "question_id": question_id,
                            "error": "Failed to load AI output",
                        }
                    )
                    continue

                # è¯»å–æ ‡å‡†ç­”æ¡ˆ
                gold_path = os.path.join(gold_dir, f"{question_id}.json")
                gold_standard = self.load_json_file(gold_path)

                if not gold_standard:
                    strategy_results["failed_cases"].append(
                        {
                            "question_id": question_id,
                            "error": "Failed to load gold standard",
                        }
                    )
                    continue

                # æ‰§è¡Œè¯„ä¼°
                evaluation_result = self.evaluate_single_case(
                    ai_output, gold_standard, question_id
                )

                # ä¿å­˜è¯„ä¼°ç»“æœåˆ°ä¸AIè¾“å‡ºåŒç›®å½•
                eval_output_path = os.path.join(
                    strategy_dir, question_id, f"{question_id}_evaluation.json"
                )
                if self.save_json_file(evaluation_result, eval_output_path):
                    self.logger.info(f"Saved evaluation result: {eval_output_path}")

                strategy_results["case_results"][question_id] = evaluation_result
                strategy_results["successful_evaluations"] += 1

                # æ”¶é›†æŒ‡æ ‡ç”¨äºè®¡ç®—å¹³å‡å€¼
                metrics = evaluation_result["metrics"]
                all_metrics["reason_similarity"].append(
                    metrics["text_similarity"]["reason_similarity"]
                )
                all_metrics["fix_similarity"].append(
                    metrics["text_similarity"]["fix_similarity"]
                )
                all_metrics["location_f1"].append(metrics["location_matching"]["f1"])
                all_metrics["top_1_accuracy"].append(
                    metrics["ranking_metrics"]["top_1_accuracy"]
                )
                all_metrics["top_3_accuracy"].append(
                    metrics["ranking_metrics"]["top_3_accuracy"]
                )
                all_metrics["map_score"].append(metrics["ranking_metrics"]["map_score"])
                all_metrics["overall_completeness"].append(
                    metrics["structure_completeness"]["overall_completeness"]
                )
                all_metrics["confidence_score"].append(metrics["confidence_score"])
                all_metrics["f1_score"].append(metrics["overall_scores"]["f1_score"])
                all_metrics["weighted_score"].append(
                    metrics["overall_scores"]["weighted_score"]
                )

                # AIç›¸ä¼¼æ€§æŒ‡æ ‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if metrics["ai_similarity"]["score"] is not None:
                    all_metrics["ai_similarity_score"].append(
                        metrics["ai_similarity"]["score"]
                    )

            except Exception as e:
                self.logger.error(f"Error evaluating {question_id}: {e}")
                strategy_results["failed_cases"].append(
                    {"question_id": question_id, "error": str(e)}
                )

        # è®¡ç®—æ±‡æ€»æŒ‡æ ‡
        if all_metrics:
            strategy_results["summary_metrics"] = {
                metric: {
                    "mean": np.mean(values),
                    "std": np.std(values),
                    "min": np.min(values),
                    "max": np.max(values),
                    "median": np.median(values),
                }
                for metric, values in all_metrics.items()
            }

        self.logger.info(
            f"Strategy {strategy_name} evaluation completed. "
            f"Success: {strategy_results['successful_evaluations']}/{strategy_results['total_cases']}"
        )

        return strategy_results

    def run_evaluation(
        self, rag_results_dir: str, gold_standard_dir: str, strategies: List[str] = None
    ) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´è¯„ä¼°

        Args:
            rag_results_dir: RAGç»“æœç›®å½•
            gold_standard_dir: æ ‡å‡†ç­”æ¡ˆç›®å½•
            strategies: è¦è¯„ä¼°çš„ç­–ç•¥åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºè¯„ä¼°æ‰€æœ‰ç­–ç•¥

        Returns:
            å®Œæ•´è¯„ä¼°ç»“æœ
        """
        self.logger.info("Starting RAG evaluation system")

        if strategies is None:
            strategies = ["basic", "compressed", "enhanced"]

        evaluation_results = {
            "evaluation_timestamp": str(Path().resolve()),
            "rag_results_dir": rag_results_dir,
            "gold_standard_dir": gold_standard_dir,
            "strategies_evaluated": strategies,
            "strategy_results": {},
            "comparative_analysis": {},
        }

        # è¯„ä¼°æ¯ä¸ªç­–ç•¥
        for strategy in strategies:
            strategy_dir = os.path.join(rag_results_dir, strategy)
            if os.path.exists(strategy_dir):
                strategy_result = self.evaluate_strategy(
                    strategy_dir, gold_standard_dir, strategy
                )
                evaluation_results["strategy_results"][strategy] = strategy_result
            else:
                self.logger.warning(f"Strategy directory not found: {strategy_dir}")

        # ç”Ÿæˆå¯¹æ¯”åˆ†æ
        evaluation_results["comparative_analysis"] = self.generate_comparative_analysis(
            evaluation_results["strategy_results"]
        )

        # ä¿å­˜å®Œæ•´è¯„ä¼°ç»“æœ
        overall_results_path = os.path.join(rag_results_dir, "evaluation_results.json")
        self.save_json_file(evaluation_results, overall_results_path)

        self.logger.info(
            f"Evaluation completed. Results saved to {overall_results_path}"
        )

        return evaluation_results

    def generate_comparative_analysis(
        self, strategy_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆç­–ç•¥å¯¹æ¯”åˆ†æ

        Args:
            strategy_results: å„ç­–ç•¥è¯„ä¼°ç»“æœ

        Returns:
            å¯¹æ¯”åˆ†æç»“æœ
        """
        if not strategy_results:
            return {}

        comparative_analysis = {
            "best_strategy_by_metric": {},
            "metric_rankings": {},
            "improvement_analysis": {},
        }

        # ä¸»è¦æŒ‡æ ‡åˆ—è¡¨
        key_metrics = [
            "f1_score",
            "weighted_score",
            "reason_similarity",
            "fix_similarity",
            "location_f1",
            "top_1_accuracy",
            "top_3_accuracy",
            "map_score",
            "overall_completeness",
            "confidence_score",
            "ai_similarity_score",
        ]

        # ä¸ºæ¯ä¸ªæŒ‡æ ‡æ‰¾å‡ºæœ€ä½³ç­–ç•¥
        for metric in key_metrics:
            best_strategy = None
            best_score = -1

            strategy_scores = {}

            for strategy, results in strategy_results.items():
                if (
                    "summary_metrics" in results
                    and metric in results["summary_metrics"]
                ):
                    score = results["summary_metrics"][metric]["mean"]
                    strategy_scores[strategy] = score

                    if score > best_score:
                        best_score = score
                        best_strategy = strategy

            if best_strategy:
                comparative_analysis["best_strategy_by_metric"][metric] = {
                    "strategy": best_strategy,
                    "score": best_score,
                    "all_scores": strategy_scores,
                }

        # è®¡ç®—æ€»ä½“æ’å
        strategy_total_scores = defaultdict(float)
        strategy_metric_counts = defaultdict(int)

        for metric, metric_data in comparative_analysis[
            "best_strategy_by_metric"
        ].items():
            for strategy, score in metric_data["all_scores"].items():
                strategy_total_scores[strategy] += score
                strategy_metric_counts[strategy] += 1

        # è®¡ç®—å¹³å‡åˆ†æ•°å¹¶æ’å
        strategy_avg_scores = {
            strategy: total_score / strategy_metric_counts[strategy]
            for strategy, total_score in strategy_total_scores.items()
        }

        comparative_analysis["metric_rankings"] = dict(
            sorted(strategy_avg_scores.items(), key=lambda x: x[1], reverse=True)
        )

        # æ”¹è¿›åˆ†æï¼ˆç›¸å¯¹äºbasicç­–ç•¥ï¼‰
        if "basic" in strategy_avg_scores:
            basic_score = strategy_avg_scores["basic"]
            for strategy, score in strategy_avg_scores.items():
                if strategy != "basic":
                    improvement = (
                        ((score - basic_score) / basic_score) * 100
                        if basic_score > 0
                        else 0
                    )
                    comparative_analysis["improvement_analysis"][strategy] = {
                        "improvement_percentage": improvement,
                        "absolute_improvement": score - basic_score,
                    }

        return comparative_analysis

    def test_ai_connection(self) -> bool:
        """
        æµ‹è¯•AI APIè¿æ¥

        Returns:
            è¿æ¥æ˜¯å¦æˆåŠŸ
        """
        if not self.ai_analysis_enabled:
            self.logger.info("AI analysis is disabled")
            return False

        try:
            self.logger.info("Testing AI API connection...")

            # å‘é€ç®€å•çš„æµ‹è¯•è¯·æ±‚
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {
                        "role": "user",
                        "content": "Hello, please respond with 'Connection test successful'",
                    }
                ],
                max_tokens=50,
                temperature=0.1,
                timeout=15.0,
            )

            result = response.choices[0].message.content.strip()
            self.logger.info(f"AI API connection test successful. Response: {result}")
            return True

        except openai.APIConnectionError as e:
            self.logger.error(f"API connection test failed: {e}")
            self.logger.info("Possible solutions:")
            self.logger.info("1. Check your internet connection")
            self.logger.info("2. Verify the API base URL is correct")
            self.logger.info("3. Check if there are any firewall restrictions")
            return False
        except openai.AuthenticationError as e:
            self.logger.error(f"Authentication failed: {e}")
            self.logger.info("Please verify your API key is correct")
            return False
        except Exception as e:
            self.logger.error(f"Connection test failed: {e}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="RAG Evaluation System")
    parser.add_argument(
        "--rag-dir",
        type=str,
        default="./results/based_on_rag",
        help="RAG results directory",
    )
    parser.add_argument(
        "--gold-dir",
        type=str,
        default="./results/gold_standard",
        help="Gold standard directory",
    )
    parser.add_argument(
        "--strategies",
        type=str,
        nargs="+",
        default=["basic", "compressed", "enhanced"],
        help="Strategies to evaluate",
    )
    parser.add_argument(
        "--api-key", type=str, help="OpenAI API key for AI similarity analysis"
    )
    parser.add_argument("--openai-base-url", type=str, help="OpenAI API base URL")
    parser.add_argument(
        "--disable-ai",
        action="store_true",
        help="Disable AI similarity analysis (faster evaluation)",
    )
    parser.add_argument(
        "--log-level",
        type=str,
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="Logging level",
    )

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.getLogger().setLevel(getattr(logging, args.log_level))

    # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    rag_dir = os.path.abspath(args.rag_dir)
    gold_dir = os.path.abspath(args.gold_dir)  # åˆ›å»ºè¯„ä¼°å™¨
    api_key = None if args.disable_ai else args.api_key
    evaluator = RAGEvaluator(api_key=api_key, openai_base_url=args.openai_base_url)

    # æµ‹è¯•AIè¿æ¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if evaluator.ai_analysis_enabled and not args.disable_ai:
        if not evaluator.test_ai_connection():
            print(
                "\nâš ï¸  AI similarity analysis will be disabled due to connection issues."
            )
            print("The evaluation will continue with other metrics only.\n")
            evaluator.ai_analysis_enabled = False
    elif args.disable_ai:
        print("\nğŸ“ AI similarity analysis disabled by user request.\n")

    # è¿è¡Œè¯„ä¼°
    results = evaluator.run_evaluation(rag_dir, gold_dir, args.strategies)

    # æ‰“å°æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("RAG EVALUATION RESULTS SUMMARY")
    print("=" * 60)

    if (
        "comparative_analysis" in results
        and "metric_rankings" in results["comparative_analysis"]
    ):
        print("\nOverall Strategy Rankings:")
        for i, (strategy, score) in enumerate(
            results["comparative_analysis"]["metric_rankings"].items(), 1
        ):
            print(f"{i}. {strategy}: {score:.4f}")

    if (
        "comparative_analysis" in results
        and "improvement_analysis" in results["comparative_analysis"]
    ):
        print("\nImprovement over Basic Strategy:")
        for strategy, analysis in results["comparative_analysis"][
            "improvement_analysis"
        ].items():
            improvement = analysis["improvement_percentage"]
            print(f"{strategy}: {improvement:+.2f}%")

    print(
        f"\nDetailed results saved to: {os.path.join(rag_dir, 'evaluation_results.json')}"
    )
    print("Individual evaluation results saved alongside AI outputs.")


if __name__ == "__main__":
    main()
